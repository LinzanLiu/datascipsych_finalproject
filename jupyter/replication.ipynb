{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ba2bb7",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "Memory judgments in item recognition tests can be influenced by familiarity-based processes like processing fluency. While traditional accounts of source memory suggest minimal impact of familiarity, recent work indicates that source memory judgments can be affected when test stimuli are processed with greater fluency due to priming. The experiments in this paper investigated the relationship between fluency and the accuracy of source memory decisions.   \n",
    "\n",
    "This analysis reproduces the findings from Experiment 1 of Huang and Shanks (2021), which aimed to investigate how fluency relates to source memory responses, including the more commonly used R/K or R/K/G responses. Another goal was to address the lack of research on memory involving multiple source attributes crossed at encoding and jointly retrieved. Participants studied words presented with different source attributes (font size and location). During testing, they identified words that clarified on screen through progressive demasking (CID-R task), made old/new and source memory judgments, and provided confidence ratings. Identification response times (RTs) from the item identification task were used as a measure of fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff3d72",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "131548ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pingouin as pg\n",
    "import sys\n",
    "sys.path.append(\"../src/project\")\n",
    "from task import load_and_preprocess_data, calculate_correctness, calculate_recognition_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c7a55",
   "metadata": {},
   "source": [
    "Load the dataset and perform data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "660b744b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Subj/Trial Number</th>\n",
       "      <th>TestTrialNum</th>\n",
       "      <th>Stimuli</th>\n",
       "      <th>idRespTime</th>\n",
       "      <th>ItemRecognResp</th>\n",
       "      <th>SourceSizeResp</th>\n",
       "      <th>SourceLocResp</th>\n",
       "      <th>ItemStatus</th>\n",
       "      <th>OldNewItemResp</th>\n",
       "      <th>ItemRecogStatus</th>\n",
       "      <th>SourceEncodingSize</th>\n",
       "      <th>SourceEncodingLoc</th>\n",
       "      <th>SourceConfidence_absSum</th>\n",
       "      <th>SourceCorrect</th>\n",
       "      <th>idRespTime_ms</th>\n",
       "      <th>size_correct</th>\n",
       "      <th>loc_correct</th>\n",
       "      <th>correct_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_10</td>\n",
       "      <td>10</td>\n",
       "      <td>tank</td>\n",
       "      <td>1.847444</td>\n",
       "      <td>Guess</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lure</td>\n",
       "      <td>old</td>\n",
       "      <td>FalseAlarm</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1847.443705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_100</td>\n",
       "      <td>100</td>\n",
       "      <td>lace</td>\n",
       "      <td>1.430649</td>\n",
       "      <td>Know</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>target</td>\n",
       "      <td>old</td>\n",
       "      <td>Hit</td>\n",
       "      <td>small</td>\n",
       "      <td>upper</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1430.648669</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_101</td>\n",
       "      <td>101</td>\n",
       "      <td>newt</td>\n",
       "      <td>1.847070</td>\n",
       "      <td>Guess</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lure</td>\n",
       "      <td>old</td>\n",
       "      <td>FalseAlarm</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1847.070295</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_102</td>\n",
       "      <td>102</td>\n",
       "      <td>kick</td>\n",
       "      <td>0.863761</td>\n",
       "      <td>Remember</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>target</td>\n",
       "      <td>old</td>\n",
       "      <td>Hit</td>\n",
       "      <td>small</td>\n",
       "      <td>lower</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>863.761488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_103</td>\n",
       "      <td>103</td>\n",
       "      <td>quart</td>\n",
       "      <td>1.147339</td>\n",
       "      <td>New</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lure</td>\n",
       "      <td>old</td>\n",
       "      <td>CorrectRej</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1147.339108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject Subj/Trial Number  TestTrialNum Stimuli  idRespTime ItemRecognResp  \\\n",
       "0     P01            P01_10            10    tank    1.847444          Guess   \n",
       "1     P01           P01_100           100    lace    1.430649           Know   \n",
       "2     P01           P01_101           101    newt    1.847070          Guess   \n",
       "3     P01           P01_102           102    kick    0.863761       Remember   \n",
       "4     P01           P01_103           103   quart    1.147339            New   \n",
       "\n",
       "   SourceSizeResp  SourceLocResp ItemStatus OldNewItemResp ItemRecogStatus  \\\n",
       "0             0.0            0.0       lure            old      FalseAlarm   \n",
       "1             3.0            3.0     target            old             Hit   \n",
       "2             1.0            1.0       lure            old      FalseAlarm   \n",
       "3             4.0            4.0     target            old             Hit   \n",
       "4             NaN            NaN       lure            old      CorrectRej   \n",
       "\n",
       "  SourceEncodingSize SourceEncodingLoc  SourceConfidence_absSum  \\\n",
       "0               None              None                      0.0   \n",
       "1              small             upper                      6.0   \n",
       "2               None              None                      2.0   \n",
       "3              small             lower                      8.0   \n",
       "4               None              None                      NaN   \n",
       "\n",
       "   SourceCorrect  idRespTime_ms  size_correct  loc_correct  correct_sources  \n",
       "0            NaN    1847.443705             0            0                0  \n",
       "1            1.0    1430.648669             0            1                1  \n",
       "2            NaN    1847.070295             0            0                0  \n",
       "3            0.0     863.761488             0            0                0  \n",
       "4            NaN    1147.339108             0            0                0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "file_path = \"../src/project/data/data_expt1.csv\"\n",
    "df = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Calculate correctness columns\n",
    "df = calculate_correctness(df)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac214e",
   "metadata": {},
   "source": [
    "### Recognition Memory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544082a",
   "metadata": {},
   "source": [
    "Calculate hit rate and false alarm rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e53ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate: 0.80\n",
      "False alarm rate: 0.25\n",
      "Total targets: 2926\n",
      "Total hits: 2329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate recognition rates\n",
    "recognition_metrics = calculate_recognition_rates(df)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Hit rate: {recognition_metrics['hit_rate']:.2f}\")\n",
    "print(f\"False alarm rate: {recognition_metrics['fa_rate']:.2f}\")\n",
    "\n",
    "# You can also access other metrics if needed\n",
    "print(f\"Total targets: {recognition_metrics['total_targets']}\")\n",
    "print(f\"Total hits: {recognition_metrics['hits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5db7c",
   "metadata": {},
   "source": [
    "Hit rate and False alarm matched the paper. Test trials receiving R, K and G were considered as ‘old’ responses. Across all valid test trials, the hit rate\n",
    " was 0.80 and the false alarm rate was 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a69824",
   "metadata": {},
   "source": [
    "### Figure 2(a) Mean item identification RTs (ms) for hits, misses, false alarms and correct rejections in Experiment 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d16ef2",
   "metadata": {},
   "source": [
    "### A two-way repeated-measures ANOVA for subjective old/new judgements and actual old/new status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c67fb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ANOVA\n",
    "# Calculate mean RTs for each subject across the four conditions\n",
    "anova_data = []\n",
    "\n",
    "for subject in df['Subject'].unique():\n",
    "    # For Hits: actual=old, judged=old\n",
    "    hit_rt = df[(df['Subject'] == subject) & (df['ItemRecogStatus'] == 'Hit')]['idRespTime'].mean()\n",
    "    \n",
    "    # For Misses: actual=old, judged=new\n",
    "    miss_rt = df[(df['Subject'] == subject) & (df['ItemRecogStatus'] == 'Miss')]['idRespTime'].mean()\n",
    "    \n",
    "    # For False Alarms: actual=new, judged=old\n",
    "    fa_rt = df[(df['Subject'] == subject) & (df['ItemRecogStatus'] == 'FalseAlarm')]['idRespTime'].mean()\n",
    "    \n",
    "    # For Correct Rejections: actual=new, judged=new\n",
    "    cr_rt = df[(df['Subject'] == subject) & (df['ItemRecogStatus'] == 'CorrectRej')]['idRespTime'].mean()\n",
    "    \n",
    "    # Add data for this subject in long format required by pingouin\n",
    "    # Actual old\n",
    "    if not np.isnan(hit_rt):\n",
    "        anova_data.append({'Subject': subject, 'ActualStatus': 'old', 'SubjectiveJudgment': 'old', 'RT': hit_rt})\n",
    "    if not np.isnan(miss_rt):\n",
    "        anova_data.append({'Subject': subject, 'ActualStatus': 'old', 'SubjectiveJudgment': 'new', 'RT': miss_rt})\n",
    "    \n",
    "    # Actual new\n",
    "    if not np.isnan(fa_rt):\n",
    "        anova_data.append({'Subject': subject, 'ActualStatus': 'new', 'SubjectiveJudgment': 'old', 'RT': fa_rt})\n",
    "    if not np.isnan(cr_rt):\n",
    "        anova_data.append({'Subject': subject, 'ActualStatus': 'new', 'SubjectiveJudgment': 'new', 'RT': cr_rt})\n",
    "\n",
    "# Convert to DataFrame for ANOVA\n",
    "anova_df = pd.DataFrame(anova_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f719fc9",
   "metadata": {},
   "source": [
    "The initial step processes experimental data from Experiment 1, extracting reaction times for each subject across four recognition conditions:\n",
    "\n",
    "Hits: old items correctly identified as \"old\"<br>\n",
    "Misses: old items incorrectly identified as \"new\"<br>\n",
    "False Alarms: new items incorrectly identified as \"old\"<br>\n",
    "Correct Rejections: new items correctly identified as \"new\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c95ae9",
   "metadata": {},
   "source": [
    "### Check data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25c35bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects with complete data: 46\n"
     ]
    }
   ],
   "source": [
    "subject_counts = anova_df.groupby('Subject').size()\n",
    "complete_subjects = subject_counts[subject_counts == 4].index\n",
    "print(f\"Number of subjects with complete data: {len(complete_subjects)}\")\n",
    "\n",
    "# Keep only subjects with complete data for balanced ANOVA\n",
    "anova_df_balanced = anova_df[anova_df['Subject'].isin(complete_subjects)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c125e",
   "metadata": {},
   "source": [
    "Before running the ANOVA, I verified data completeness by:\n",
    "\n",
    "Counting how many conditions each subject completed\n",
    "Selecting only subjects with data for all four conditions\n",
    "Creating a balanced dataset for valid repeated-measures analysis\n",
    "\n",
    "The analysis included 46 subjects with complete data across all conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bae949",
   "metadata": {},
   "source": [
    "### Run the two-way repeated-measures ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60db7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ananconda\\lib\\site-packages\\pingouin\\distribution.py:507: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  data.groupby(level=1, axis=1, observed=True, group_keys=False)\n",
      "d:\\Ananconda\\lib\\site-packages\\pingouin\\distribution.py:507: FutureWarning: DataFrameGroupBy.diff with axis=1 is deprecated and will be removed in a future version. Operate on the un-grouped DataFrame instead\n",
      "  data.groupby(level=1, axis=1, observed=True, group_keys=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>ddof1</th>\n",
       "      <th>ddof2</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>p-GG-corr</th>\n",
       "      <th>ng2</th>\n",
       "      <th>eps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActualStatus</td>\n",
       "      <td>1.6884</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1.6884</td>\n",
       "      <td>57.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0644</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SubjectiveJudgment</td>\n",
       "      <td>0.4534</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.4534</td>\n",
       "      <td>7.8244</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActualStatus * SubjectiveJudgment</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.7120</td>\n",
       "      <td>0.7120</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Source      SS  ddof1  ddof2      MS        F  \\\n",
       "0                       ActualStatus  1.6884      1     45  1.6884  57.0000   \n",
       "1                 SubjectiveJudgment  0.4534      1     45  0.4534   7.8244   \n",
       "2  ActualStatus * SubjectiveJudgment  0.0046      1     45  0.0046   0.1380   \n",
       "\n",
       "    p-unc  p-GG-corr     ng2  eps  \n",
       "0  0.0000     0.0000  0.0644  1.0  \n",
       "1  0.0076     0.0076  0.0181  1.0  \n",
       "2  0.7120     0.7120  0.0002  1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the two-way repeated-measures ANOVA\n",
    "aov = pg.rm_anova(\n",
    "    data=anova_df_balanced,\n",
    "    dv='RT',\n",
    "    within=['ActualStatus', 'SubjectiveJudgment'],\n",
    "    subject='Subject',\n",
    "    detailed=True\n",
    ").round(4)\n",
    "\n",
    "aov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40cd89",
   "metadata": {},
   "source": [
    "The two-way repeated-measures ANOVA examined:\n",
    "\n",
    "Effect of actual item status (old vs. new)\n",
    "\n",
    "Effect of subjective judgment (judged old vs. judged new)\n",
    "\n",
    "Potential interaction between these factors\n",
    "\n",
    "Results showed:\n",
    "\n",
    "Main effect of actual status: F(1, 45) = 57.00, p < 0.0001, η²p = 0.064\n",
    "Main effect of subjective judgment: F(1, 45) = 7.82, p = 0.0076, η²p = 0.018\n",
    "No significant interaction: F(1, 45) = 0.14, p = 0.712, η²p = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fede3",
   "metadata": {},
   "source": [
    "### Compute means for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30b6b092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean RT</th>\n",
       "      <th>SD</th>\n",
       "      <th>Count</th>\n",
       "      <th>Mean RT (ms)</th>\n",
       "      <th>SD (ms)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ActualStatus</th>\n",
       "      <th>SubjectiveJudgment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">new</th>\n",
       "      <th>new</th>\n",
       "      <td>1.8790</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>46</td>\n",
       "      <td>1879.0030</td>\n",
       "      <td>350.8597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>1.7697</td>\n",
       "      <td>0.3842</td>\n",
       "      <td>46</td>\n",
       "      <td>1769.7065</td>\n",
       "      <td>384.1544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">old</th>\n",
       "      <th>new</th>\n",
       "      <td>1.6774</td>\n",
       "      <td>0.3881</td>\n",
       "      <td>46</td>\n",
       "      <td>1677.3968</td>\n",
       "      <td>388.0513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>1.5881</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>46</td>\n",
       "      <td>1588.1421</td>\n",
       "      <td>351.9480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Mean RT      SD  Count  Mean RT (ms)  \\\n",
       "ActualStatus SubjectiveJudgment                                         \n",
       "new          new                  1.8790  0.3509     46     1879.0030   \n",
       "             old                  1.7697  0.3842     46     1769.7065   \n",
       "old          new                  1.6774  0.3881     46     1677.3968   \n",
       "             old                  1.5881  0.3519     46     1588.1421   \n",
       "\n",
       "                                  SD (ms)  \n",
       "ActualStatus SubjectiveJudgment            \n",
       "new          new                 350.8597  \n",
       "             old                 384.1544  \n",
       "old          new                 388.0513  \n",
       "             old                 351.9480  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_means = anova_df_balanced.groupby(['ActualStatus', 'SubjectiveJudgment'])['RT'].agg(['mean', 'std', 'count'])\n",
    "condition_means.columns = ['Mean RT', 'SD', 'Count']\n",
    "\n",
    "# Convert to milliseconds for easier interpretation\n",
    "condition_means['Mean RT (ms)'] = condition_means['Mean RT'] * 1000\n",
    "condition_means['SD (ms)'] = condition_means['SD'] * 1000\n",
    "\n",
    "condition_means.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba724159",
   "metadata": {},
   "source": [
    "The condition means table reveals:\n",
    "\n",
    "Fastest RTs for \"old-old\" condition (hits): 1588 ms <br>\n",
    "Second fastest for \"old-new\" (misses): 1677 ms<br>\n",
    "Third fastest for \"new-old\" (false alarms): 1770 ms<br>\n",
    "Slowest for \"new-new\" (correct rejections): 1879 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401e264",
   "metadata": {},
   "source": [
    "### Prepare data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b124cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['hit', 'miss', 'false alarm', 'correct rejection']\n",
    "item_recog_map = {'Hit': 'hit', 'Miss': 'miss', 'FalseAlarm': 'false alarm', 'CorrectRej': 'correct rejection'}\n",
    "means = []\n",
    "errors = []\n",
    "labels = []\n",
    "\n",
    "for status, category in item_recog_map.items():\n",
    "    if status in df['ItemRecogStatus'].unique():\n",
    "        data = df[df['ItemRecogStatus'] == status]['idRespTime_ms']\n",
    "        means.append(data.mean())\n",
    "        errors.append(1.96 * stats.sem(data))\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a3933ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "means = np.array(means)\n",
    "errors = np.array(errors)\n",
    "x_positions = np.arange(len(means))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bars = plt.bar(\n",
    "    x_positions, \n",
    "    means, \n",
    "    edgecolor='black',\n",
    "    width=0.6\n",
    ")\n",
    "\n",
    "# Add error bars using numpy arrays\n",
    "plt.errorbar(\n",
    "    x_positions, \n",
    "    means, \n",
    "    yerr=errors, \n",
    "    fmt='none', \n",
    "    color='black', \n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "# Set styling\n",
    "plt.ylim(1000, 2000)\n",
    "plt.xticks(x_positions, labels, rotation=0) \n",
    "plt.xlabel('item recognition category')\n",
    "plt.ylabel('mean RT (ms)')\n",
    "plt.savefig('figure_2a.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce865f3f",
   "metadata": {},
   "source": [
    "The bar chart visualization reproduces Figure 2(a) from the paper, showing mean identification RTs across the four recognition categories with 95% confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8139788",
   "metadata": {},
   "source": [
    "## Conclusion for Figure 2(a)\n",
    "\n",
    "The analysis reveals two significant main effects:\n",
    "1. **Actual status effect**: F(1,45) = 57.00, p < 0.0001, η²p = 0.064\n",
    "2. **Subjective judgment effect**: F(1,45) = 7.82, p = 0.0076, η²p = 0.018\n",
    "\n",
    "The data shows a systematic pattern of identification RTs across recognition categories, with hits being identified fastest (M = 1588 ms), followed by misses (M = 1677 ms), false alarms (M = 1770 ms), and correct rejections (M = 1831 ms). This pattern matches the shown figure in the original paper, indicating that correct recognition of old items (hits) is associated with the fastest identification times, while correctly rejecting new items takes the longest.\n",
    "\n",
    "This pattern suggests that both the actual status of items and participants' subjective judgments influence processing fluency, as measured by identification RTs. Items judged as \"old\" were identified more quickly than those judged as \"new,\" regardless of their actual status, supporting the paper's hypothesis that familiarity-based processes are related to recognition memory performance. The faster response times for hits compared to correct rejections suggest that previously studied items benefit from perceptual fluency, making them easier and faster to identify.\n",
    "\n",
    "The non-significant interaction (F(1,45) = 0.14, p = 0.71) indicates that the effect of subjective judgment on identification RTs is similar for both old and new items, consistent with a signal-detection account of recognition memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0db87",
   "metadata": {},
   "source": [
    "## Figure 2(b) Mean identification RTs for item hit trials according to ‘remember’, ‘know’, ‘guess’ and ‘new’ response categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b60294",
   "metadata": {},
   "source": [
    "### Filter hit trials and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e097853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for hit trials only\n",
    "hit_trials = df[df['ItemRecogStatus'] == 'Hit']\n",
    "\n",
    "# Calculate mean RT for each subject by response type (R, K, G)\n",
    "rkg_data = []\n",
    "\n",
    "for subject in hit_trials['Subject'].unique():\n",
    "    # Get data for this subject\n",
    "    subject_data = hit_trials[hit_trials['Subject'] == subject]\n",
    "    \n",
    "    # Calculate mean RT for each response type\n",
    "    for resp_type in ['Remember', 'Know', 'Guess']:\n",
    "        resp_data = subject_data[subject_data['ItemRecognResp'] == resp_type]\n",
    "        if len(resp_data) > 0:  \n",
    "            mean_rt = resp_data['idRespTime'].mean()\n",
    "            rkg_data.append({\n",
    "                'Subject': subject,\n",
    "                'ResponseType': resp_type,\n",
    "                'RT': mean_rt\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "rkg_df = pd.DataFrame(rkg_data)\n",
    "\n",
    "# Check how many subjects have data for all three response types\n",
    "response_counts = rkg_df.groupby('Subject').size()\n",
    "complete_subjects = response_counts[response_counts == 3].index\n",
    "complete_subjects_count = len(complete_subjects)\n",
    "complete_subjects_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a25689",
   "metadata": {},
   "source": [
    "For this analysis, I focused specifically on hit trials (correctly recognized old items) and examined how identification RTs vary across different subjective memory judgments:\n",
    "\n",
    "Remember (R): Items accompanied by conscious recollection of the study event<br>\n",
    "Know (K): Items recognized without conscious recollection of the study episode<br>\n",
    "Guess (G): Items where participants guessed they were old\n",
    "\n",
    "I extracted mean RTs for each subject and response type (R/K/G) and found that 43 subjects had data for all three response categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab72e2d",
   "metadata": {},
   "source": [
    "### Run the one-way repeated ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ab69b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>ddof1</th>\n",
       "      <th>ddof2</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>ng2</th>\n",
       "      <th>eps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResponseType</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>10.3072</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.9488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Source  ddof1  ddof2        F   p-unc     ng2     eps\n",
       "0  ResponseType      2     84  10.3072  0.0001  0.0344  0.9488"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only subjects with complete data\n",
    "rkg_balanced = rkg_df[rkg_df['Subject'].isin(complete_subjects)]\n",
    "\n",
    "# Run the ANOVA\n",
    "aov2 = pg.rm_anova(\n",
    "    data=rkg_balanced,\n",
    "    dv='RT',\n",
    "    within='ResponseType',\n",
    "    subject='Subject',\n",
    "    #detailed=True\n",
    ").round(4)\n",
    "\n",
    "aov2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec632e3",
   "metadata": {},
   "source": [
    "A one-way repeated-measures ANOVA revealed a significant main effect of response type on identification RTs (F(2, 84) = 10.3072, p < 0.0001, η²p = 0.034), indicating that the subjective memory experiences were associated with different levels of processing fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f219ef4",
   "metadata": {},
   "source": [
    "### Run post-hoc tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5e408de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contrast</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Paired</th>\n",
       "      <th>Parametric</th>\n",
       "      <th>T</th>\n",
       "      <th>dof</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>p-corr</th>\n",
       "      <th>p-adjust</th>\n",
       "      <th>BF10</th>\n",
       "      <th>hedges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResponseType</td>\n",
       "      <td>Guess</td>\n",
       "      <td>Know</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.16</td>\n",
       "      <td>42.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>bonf</td>\n",
       "      <td>1.344</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ResponseType</td>\n",
       "      <td>Guess</td>\n",
       "      <td>Remember</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4.12</td>\n",
       "      <td>42.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bonf</td>\n",
       "      <td>143.904</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ResponseType</td>\n",
       "      <td>Know</td>\n",
       "      <td>Remember</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.67</td>\n",
       "      <td>42.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>bonf</td>\n",
       "      <td>3.713</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Contrast      A         B  Paired  Parametric     T   dof alternative  \\\n",
       "0  ResponseType  Guess      Know    True        True  2.16  42.0   two-sided   \n",
       "1  ResponseType  Guess  Remember    True        True  4.12  42.0   two-sided   \n",
       "2  ResponseType   Know  Remember    True        True  2.67  42.0   two-sided   \n",
       "\n",
       "   p-unc  p-corr p-adjust     BF10  hedges  \n",
       "0   0.04    0.11     bonf    1.344    0.20  \n",
       "1   0.00    0.00     bonf  143.904    0.46  \n",
       "2   0.01    0.03     bonf    3.713    0.25  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run post-hoc tests with Bonferroni correction\n",
    "posthoc = pg.pairwise_tests(\n",
    "    data=rkg_balanced,\n",
    "    dv='RT',\n",
    "    within='ResponseType',\n",
    "    subject='Subject',\n",
    "    padjust='bonf'\n",
    ").round(2)\n",
    "\n",
    "posthoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221938aa",
   "metadata": {},
   "source": [
    "Post-hoc pairwise comparisons with Bonferroni correction showed significant differences between all three response types: Remember vs. Know (t(42) = 2.67, p = 0.01, BF10 = 3.71), Remember vs. Guess (t(42) = 4.12, p < 0.001, BF10 = 143.90), and Know vs. Guess (t(42) = 2.16, p = 0.04, BF10 = 1.34)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d09581",
   "metadata": {},
   "source": [
    "### Calculate means for each response type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40e1251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean RT</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Count</th>\n",
       "      <th>Mean RT (ms)</th>\n",
       "      <th>Std Dev (ms)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ResponseType</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Guess</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0.44</td>\n",
       "      <td>43</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know</th>\n",
       "      <td>1.63</td>\n",
       "      <td>0.42</td>\n",
       "      <td>43</td>\n",
       "      <td>1630.0</td>\n",
       "      <td>420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Remember</th>\n",
       "      <td>1.53</td>\n",
       "      <td>0.35</td>\n",
       "      <td>43</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mean RT  Std Dev  Count  Mean RT (ms)  Std Dev (ms)\n",
       "ResponseType                                                     \n",
       "Guess            1.71     0.44     43        1710.0         440.0\n",
       "Know             1.63     0.42     43        1630.0         420.0\n",
       "Remember         1.53     0.35     43        1530.0         350.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = rkg_balanced.groupby('ResponseType')['RT'].agg(['mean', 'std', 'count']).round(2)\n",
    "means.columns = ['Mean RT', 'Std Dev', 'Count']\n",
    "means['Mean RT (ms)'] = means['Mean RT'] * 1000\n",
    "means['Std Dev (ms)'] = means['Std Dev'] * 1000\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d18d49",
   "metadata": {},
   "source": [
    "The mean identification RTs across response types. This shows a clear gradient where Remember responses were associated with the fastest identification times, followed by Know responses, Guess responses, and finally New responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8075e0",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ae6cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type\tMean RT (ms)\t95% CI\n",
      "remember\t1503.04\t±30.89\n",
      "know\t1597.16\t±52.54\n",
      "guess\t1698.86\t±73.56\n",
      "new\t1792.08\t±25.48\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for R/K/G responses\n",
    "order_rkg = ['remember', 'know', 'guess', 'new'] \n",
    "item_resp_map = {'Remember': 'remember', 'Know': 'know', 'Guess': 'guess', 'New': 'new'}\n",
    "\n",
    "# Get all hit trials for R/K/G\n",
    "hit_trials = df[df['ItemRecogStatus'] == 'Hit']\n",
    "\n",
    "# Use all trials where ItemRecognResp is New\n",
    "new_trials = df[df['ItemRecognResp'] == 'New']\n",
    "\n",
    "means = []\n",
    "errors = []\n",
    "labels = []\n",
    "order_vals = []\n",
    "\n",
    "# Process R/K/G from hit trials\n",
    "for resp, category in item_resp_map.items():\n",
    "    if resp == 'New':\n",
    "        # Handle New responses separately\n",
    "        data = new_trials['idRespTime_ms']\n",
    "    else:\n",
    "        # Handle R/K/G from hit trials\n",
    "        data = hit_trials[hit_trials['ItemRecognResp'] == resp]['idRespTime_ms']\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        means.append(data.mean())\n",
    "        errors.append(1.96 * stats.sem(data))\n",
    "        labels.append(category)\n",
    "        order_vals.append(order_rkg.index(category))\n",
    "\n",
    "# Sort by order values\n",
    "ordered_indices = np.argsort(order_vals)\n",
    "means = np.array(means)[ordered_indices]\n",
    "errors = np.array(errors)[ordered_indices]\n",
    "labels = [labels[i] for i in ordered_indices]\n",
    "x_positions = np.arange(len(means))\n",
    "\n",
    "# Print out the calculated means and errors\n",
    "print(\"Response Type\\tMean RT (ms)\\t95% CI\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label}\\t{means[i]:.2f}\\t±{errors[i]:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the bar plot\n",
    "bars = plt.bar(\n",
    "    x_positions, \n",
    "    means, \n",
    "    edgecolor='black',\n",
    "    width=0.6\n",
    ")\n",
    "\n",
    "# Add error bars using numpy arrays\n",
    "plt.errorbar(\n",
    "    x_positions, \n",
    "    means, \n",
    "    yerr=errors, \n",
    "    fmt='none', \n",
    "    color='black', \n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "# Set styling\n",
    "plt.ylim(1000, 2000)\n",
    "plt.xticks(x_positions, labels, rotation=45, ha='right')\n",
    "plt.xlabel('recognition judgement')\n",
    "plt.ylabel('mean RT (ms)')\n",
    "plt.savefig('figure_2b.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c507509",
   "metadata": {},
   "source": [
    "The bar chart visualization reproduces Figure 2(b) from the paper, showing mean identification RTs across the four response categories with 95% confidence intervals. The pattern shows a systematic increase in RTs from Remember to New judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87750ec",
   "metadata": {},
   "source": [
    "## Conclusion for Figure 2(b)\n",
    "The analysis confirms the findings from the paper, demonstrating a significant effect of subjective memory judgments on identification RTs. Items that participants subjectively experienced as \"remembered\" (with conscious recollection) were identified significantly faster than those they merely \"knew\" were familiar, which in turn were identified faster than those they \"guessed\" were old.\n",
    "This systematic gradient in identification RTs across R/K/G responses supports the relationship between processing fluency and different subjective states of memory. The strong Bayes Factor (BF10 = 143.90) for the Remember-Guess comparison indicates particularly substantial evidence for this difference.\n",
    "The results suggest that the subjective experience of remembering is associated with greater processing fluency than knowing or guessing, which aligns with theoretical accounts proposing that processing fluency contributes to both familiarity and recollection, albeit potentially to different degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d156d9",
   "metadata": {},
   "source": [
    "## Source Memory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f178d",
   "metadata": {},
   "source": [
    "## Figure 3： Mean identification RTs (ms) for recognition hit trials with correct source judgements on both, one or none of the source dimensions in Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c9c16",
   "metadata": {},
   "source": [
    "For this analysis, I examined how identification RTs vary based on the accuracy of source memory judgments for hit trials (correctly recognized old items). The categorized trials by the number of source dimensions (font size and location) correctly identified:\n",
    "\n",
    "Both correct: Trials where participants correctly identified both the font size and location\n",
    "\n",
    "One correct: Trials where only one source dimension was correctly identified\n",
    "\n",
    "Neither correct: Trials where neither source dimension was correctly identified\n",
    "\n",
    "My analysis first compared RT differences between correct and incorrect judgments for each source dimension separately, then examined the overall pattern across number of correct source dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd8e3e",
   "metadata": {},
   "source": [
    "### Calculate RT differences for size and location dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5974d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_diff_data = []\n",
    "for subject in df['Subject'].unique():\n",
    "    subject_data = df[df['Subject'] == subject]\n",
    "    hit_trials = subject_data[subject_data['ItemRecogStatus'] == 'Hit']\n",
    "    \n",
    "    # Size dimension\n",
    "    size_correct = hit_trials[hit_trials['size_correct'] == 1]['idRespTime']\n",
    "    size_incorrect = hit_trials[hit_trials['size_correct'] == 0]['idRespTime']\n",
    "    if len(size_correct) > 0 and len(size_incorrect) > 0:\n",
    "        size_diff = size_correct.mean() - size_incorrect.mean()\n",
    "        size_diff_ms = size_diff * 1000\n",
    "    else:\n",
    "        size_diff_ms = np.nan\n",
    "        \n",
    "    # Location dimension\n",
    "    loc_correct = hit_trials[hit_trials['loc_correct'] == 1]['idRespTime']\n",
    "    loc_incorrect = hit_trials[hit_trials['loc_correct'] == 0]['idRespTime']\n",
    "    if len(loc_correct) > 0 and len(loc_incorrect) > 0:\n",
    "        loc_diff = loc_correct.mean() - loc_incorrect.mean()\n",
    "        loc_diff_ms = loc_diff * 1000\n",
    "    else:\n",
    "        loc_diff_ms = np.nan\n",
    "    \n",
    "    rt_diff_data.append({\n",
    "        'Subject': subject,\n",
    "        'SizeDiff': size_diff_ms,\n",
    "        'LocDiff': loc_diff_ms\n",
    "    })\n",
    "\n",
    "rt_diff_df = pd.DataFrame(rt_diff_data)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "rt_diff_df_complete = rt_diff_df.dropna()\n",
    "len(rt_diff_df_complete)  # Number of subjects with complete data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35b78c",
   "metadata": {},
   "source": [
    "All 48 participants completed the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceba3de",
   "metadata": {},
   "source": [
    "### Paired t-test comparing size vs location RT difference scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "701ded26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>dof</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p-val</th>\n",
       "      <th>CI95%</th>\n",
       "      <th>cohen-d</th>\n",
       "      <th>BF10</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T-test</th>\n",
       "      <td>-0.590415</td>\n",
       "      <td>47</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.557741</td>\n",
       "      <td>[-77.03, 42.07]</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.070058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               T  dof alternative     p-val            CI95%  cohen-d   BF10  \\\n",
       "T-test -0.590415   47   two-sided  0.557741  [-77.03, 42.07]   0.0613  0.185   \n",
       "\n",
       "           power  \n",
       "T-test  0.070058  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run paired t-test comparing size vs location RT difference scores\n",
    "ttest_result = pg.ttest(rt_diff_df_complete['SizeDiff'], \n",
    "                       rt_diff_df_complete['LocDiff'], \n",
    "                       paired=True)\n",
    "\n",
    "\n",
    "ttest_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92f6bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2921106496.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bf10 = pg.bayesfactor_ttest(ttest_result['T'][0], len(rt_diff_df_complete), paired=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18493917293135376"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate descriptive statistics\n",
    "size_mean = rt_diff_df_complete['SizeDiff'].mean()\n",
    "size_sd = rt_diff_df_complete['SizeDiff'].std()\n",
    "loc_mean = rt_diff_df_complete['LocDiff'].mean()\n",
    "loc_sd = rt_diff_df_complete['LocDiff'].std()\n",
    "\n",
    "# Create a DataFrame for easier viewing\n",
    "desc_stats = pd.DataFrame({\n",
    "    'Dimension': ['Size', 'Location'],\n",
    "    'Mean (ms)': [size_mean, loc_mean],\n",
    "    'SD (ms)': [size_sd, loc_sd]\n",
    "})\n",
    "desc_stats\n",
    "\n",
    "# Calculate Bayes Factor\n",
    "bf10 = pg.bayesfactor_ttest(ttest_result['T'][0], len(rt_diff_df_complete), paired=True)\n",
    "bf10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b82cc",
   "metadata": {},
   "source": [
    "### Prepare data for ANOVA on number of correct sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1786a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ANOVA by number of correct sources\n",
    "anova_data = []\n",
    "\n",
    "for subject in df['Subject'].unique():\n",
    "    subject_data = df[df['Subject'] == subject]\n",
    "    hit_trials = subject_data[subject_data['ItemRecogStatus'] == 'Hit']\n",
    "    \n",
    "    # Group by number of correct sources (0, 1, 2)\n",
    "    for num_correct in [0, 1, 2]:\n",
    "        trials = hit_trials[hit_trials['correct_sources'] == num_correct]\n",
    "        if len(trials) > 0:\n",
    "            mean_rt = trials['idRespTime'].mean()\n",
    "            anova_data.append({\n",
    "                'Subject': subject,\n",
    "                'NumCorrectSources': str(num_correct),  \n",
    "                'RT': mean_rt\n",
    "            })\n",
    "\n",
    "anova_df = pd.DataFrame(anova_data)\n",
    "\n",
    "# Check how many subjects have data for all three conditions\n",
    "subject_counts = anova_df.groupby('Subject').size()\n",
    "complete_subjects = subject_counts[subject_counts == 3].index\n",
    "len(complete_subjects)  # Number of subjects with data for all three source conditions\n",
    "\n",
    "# Keep only subjects with complete data\n",
    "anova_df_balanced = anova_df[anova_df['Subject'].isin(complete_subjects)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd795ce",
   "metadata": {},
   "source": [
    "### Run one-way repeated measures ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d59fbeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>ddof1</th>\n",
       "      <th>ddof2</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>p-GG-corr</th>\n",
       "      <th>ng2</th>\n",
       "      <th>eps</th>\n",
       "      <th>sphericity</th>\n",
       "      <th>W-spher</th>\n",
       "      <th>p-spher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NumCorrectSources</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>5.5003</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>False</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Source  ddof1  ddof2       F   p-unc  p-GG-corr     ng2     eps  \\\n",
       "0  NumCorrectSources      2     92  5.5003  0.0055     0.0127  0.0353  0.7128   \n",
       "\n",
       "   sphericity  W-spher  p-spher  \n",
       "0       False    0.597      0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aov3 = pg.rm_anova(\n",
    "    data=anova_df_balanced,\n",
    "    dv='RT',\n",
    "    within='NumCorrectSources',\n",
    "    subject='Subject',\n",
    "    #detailed=True\n",
    ").round(4)\n",
    "\n",
    "aov3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa650b6f",
   "metadata": {},
   "source": [
    "A one-way repeated-measures ANOVA with number of correct sources (0, 1, or 2) as the within-subjects factor and identification RT as the dependent variable. The analysis revealed a significant effect of source accuracy on identification speed, demonstrating that the number of correctly remembered source dimensions is systematically related to processing fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143cd2d",
   "metadata": {},
   "source": [
    "### Run post-hoc pairwise comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abce03b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ananconda\\lib\\site-packages\\pingouin\\pairwise.py:28: UserWarning: pairwise_ttests is deprecated, use pairwise_tests instead.\n",
      "  warnings.warn(\"pairwise_ttests is deprecated, use pairwise_tests instead.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contrast</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Paired</th>\n",
       "      <th>Parametric</th>\n",
       "      <th>T</th>\n",
       "      <th>dof</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>p-corr</th>\n",
       "      <th>p-adjust</th>\n",
       "      <th>BF10</th>\n",
       "      <th>hedges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NumCorrectSources</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.7254</td>\n",
       "      <td>46.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>bonf</td>\n",
       "      <td>4.188</td>\n",
       "      <td>0.2651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NumCorrectSources</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6638</td>\n",
       "      <td>46.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>bonf</td>\n",
       "      <td>3.653</td>\n",
       "      <td>0.4263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NumCorrectSources</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.4848</td>\n",
       "      <td>46.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4332</td>\n",
       "      <td>bonf</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.2133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Contrast  A  B  Paired  Parametric       T   dof alternative  \\\n",
       "0  NumCorrectSources  0  1    True        True  2.7254  46.0   two-sided   \n",
       "1  NumCorrectSources  0  2    True        True  2.6638  46.0   two-sided   \n",
       "2  NumCorrectSources  1  2    True        True  1.4848  46.0   two-sided   \n",
       "\n",
       "    p-unc  p-corr p-adjust   BF10  hedges  \n",
       "0  0.0091  0.0272     bonf  4.188  0.2651  \n",
       "1  0.0106  0.0318     bonf  3.653  0.4263  \n",
       "2  0.1444  0.4332     bonf   0.44  0.2133  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posthoc = pg.pairwise_ttests(\n",
    "    data=anova_df_balanced,\n",
    "    dv='RT',\n",
    "    within='NumCorrectSources',\n",
    "    subject='Subject',\n",
    "    padjust='bonf'\n",
    ").round(4)\n",
    "\n",
    "posthoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96beb069",
   "metadata": {},
   "source": [
    "These analyses helped identify which specific pairs of conditions differed significantly, revealing that trials with both correct sources differed significantly from trials with neither correct, as did trials with one correct source versus neither correct. The results here are also consistent with the description in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200511",
   "metadata": {},
   "source": [
    "### Calculate mean for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14ecbc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean RTs by number of correct sources:\n",
      "0 correct sources: 1642.9986 (±111.0369)\n",
      "1 correct sources: 1549.8715 (±86.6517)\n",
      "2 correct sources: 1473.5530 (±114.3595)\n"
     ]
    }
   ],
   "source": [
    "means = anova_df_balanced.groupby('NumCorrectSources')['RT'].mean()*1000\n",
    "errors = anova_df_balanced.groupby('NumCorrectSources')['RT'].sem() * 1.96*1000\n",
    "    \n",
    "print(\"\\nMean RTs by number of correct sources:\")\n",
    "for num_sources in ['0', '1', '2']:\n",
    "    if num_sources in means.index:\n",
    "        print(f\"{num_sources} correct sources: {means[num_sources]:.4f} (±{errors[num_sources]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45c51f",
   "metadata": {},
   "source": [
    "This demonstrates a clear pattern where identification RTs decrease as more source dimensions are correctly remembered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2e937",
   "metadata": {},
   "source": [
    "### Visualization of Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c31802b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate means and confidence intervals for each condition\n",
    "means = anova_df_balanced.groupby('NumCorrectSources')['RT'].mean()\n",
    "errors = anova_df_balanced.groupby('NumCorrectSources')['RT'].sem() * 1.96\n",
    "\n",
    "# Create a DataFrame for display\n",
    "mean_stats = pd.DataFrame({\n",
    "    'Number of correct sources': ['0', '1', '2'],\n",
    "    'Mean RT': [means['0'], means['1'], means['2']],\n",
    "    '95% CI': [errors['0'], errors['1'], errors['2']]\n",
    "})\n",
    "mean_stats\n",
    "\n",
    "# Create Figure 3\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create bar plot in reverse order (both, one, neither)\n",
    "x_pos = np.arange(3)\n",
    "x_labels = ['both correct', 'one correct', 'neither correct']\n",
    "\n",
    "plt.bar(\n",
    "    x_pos, \n",
    "    [means['2'], means['1'], means['0']], \n",
    "    edgecolor='black',\n",
    "    width=0.6\n",
    ")\n",
    "\n",
    "plt.errorbar(\n",
    "    x_pos, \n",
    "    [means['2'], means['1'], means['0']], \n",
    "    yerr=[errors['2'], errors['1'], errors['0']], \n",
    "    fmt='none', \n",
    "    color='black', \n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "# Convert to milliseconds for the y-axis (multiply by 1000)\n",
    "y_ticks = plt.yticks()[0]\n",
    "plt.yticks(y_ticks, [f'{y*1000:.0f}' for y in y_ticks])\n",
    "\n",
    "plt.ylim(1.0, 2.0)  # Adjust based on your data\n",
    "plt.xticks(x_pos, x_labels)\n",
    "plt.ylabel('mean RT (ms)')\n",
    "plt.xlabel('no. source dimensions correct')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_3.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9beded",
   "metadata": {},
   "source": [
    "The figure 3 shows RTs increasing as source accuracy decreases: both correct, one correct, neither correct. This aligns with the paper's finding that identification is faster when source information is accurately remembered. This result is particularly important for the argument of this paper, which argues that fluency (measured by recognition speed) is related to source memory accuracy. This suggests that the retrieval of contextual details (source information) is associated with improved processing fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73584326",
   "metadata": {},
   "source": [
    "## Conclusion for figure 3\n",
    "The analysis confirms the findings from Huang & Shanks (2021), showing that identification RTs decrease systematically as source memory accuracy increases. Trials with both sources correct showed the fastest identification times (1473.6 ms), followed by trials with one source correct (1549.9 ms), and trials with neither source correct had the slowest RTs (1643.0 ms).\n",
    "This pattern provides evidence that processing fluency (indexed by identification speed) is related to source memory accuracy. The significant differences between conditions, supported by moderate Bayes factors (BF10 > 3), suggest that familiarity-based processes may contribute to accurate source memory judgments. These findings challenge conventional dual-process accounts that assume source memory judgments rely exclusively on recollection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e69fe",
   "metadata": {},
   "source": [
    "## Table 1 - Source accuracy frequencies and confidence ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f2f24",
   "metadata": {},
   "source": [
    "###  Filter hit trials and prepare frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3336c21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>correct_sources</th>\n",
       "      <th>neither_source_correct</th>\n",
       "      <th>one_source_correct</th>\n",
       "      <th>both_sources_correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemRecognResp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Remember</th>\n",
       "      <td>412</td>\n",
       "      <td>510</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know</th>\n",
       "      <td>243</td>\n",
       "      <td>199</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guess</th>\n",
       "      <td>225</td>\n",
       "      <td>66</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "correct_sources  neither_source_correct  one_source_correct  \\\n",
       "ItemRecognResp                                                \n",
       "Remember                            412                 510   \n",
       "Know                                243                 199   \n",
       "Guess                               225                  66   \n",
       "\n",
       "correct_sources  both_sources_correct  \n",
       "ItemRecognResp                         \n",
       "Remember                          509  \n",
       "Know                              122  \n",
       "Guess                              43  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_trials = df[df['ItemRecogStatus'] == 'Hit']\n",
    "\n",
    "# Count frequencies by response type and number of correct sources\n",
    "freq_table = pd.crosstab(\n",
    "    hit_trials['ItemRecognResp'], \n",
    "    hit_trials['correct_sources']\n",
    ").rename(columns={0: 'neither_source_correct', 1: 'one_source_correct', 2: 'both_sources_correct'})\n",
    "\n",
    "# Reorder to match paper's convention\n",
    "if 'Remember' in freq_table.index and 'Know' in freq_table.index and 'Guess' in freq_table.index:\n",
    "    freq_table = freq_table.loc[['Remember', 'Know', 'Guess']]\n",
    "\n",
    "# Display the frequency table\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3dc5b",
   "metadata": {},
   "source": [
    "It extracts hit trials (correctly recognized old items) and creates a cross-tabulation of response types (Remember/Know/Guess) against the number of correct source dimensions (0, 1, or 2). The resulting frequency table provides a clear view of how memory judgments relate to source accuracy, showing that Remember responses are more often associated with correct source information than Know or Guess responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220a735",
   "metadata": {},
   "source": [
    "### Calculate confidence statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39a53cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseType</th>\n",
       "      <th>MeanConfidence</th>\n",
       "      <th>CI_Lower</th>\n",
       "      <th>CI_Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember</td>\n",
       "      <td>3.994410</td>\n",
       "      <td>3.862462</td>\n",
       "      <td>4.126357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Know</td>\n",
       "      <td>2.593972</td>\n",
       "      <td>2.423359</td>\n",
       "      <td>2.764585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guess</td>\n",
       "      <td>1.293413</td>\n",
       "      <td>1.116979</td>\n",
       "      <td>1.469847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ResponseType  MeanConfidence  CI_Lower  CI_Upper\n",
       "0     Remember        3.994410  3.862462  4.126357\n",
       "1         Know        2.593972  2.423359  2.764585\n",
       "2        Guess        1.293413  1.116979  1.469847"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_stats = []\n",
    "\n",
    "for resp_type in ['Remember', 'Know', 'Guess']:\n",
    "    if resp_type in hit_trials['ItemRecognResp'].unique():\n",
    "        conf_values = hit_trials[hit_trials['ItemRecognResp'] == resp_type]['SourceConfidence_absSum']\n",
    "        mean_conf = conf_values.mean()\n",
    "        conf_sem = conf_values.sem()\n",
    "        ci_lower = mean_conf - (1.96 * conf_sem)\n",
    "        ci_upper = mean_conf + (1.96 * conf_sem)\n",
    "        conf_stats.append({\n",
    "            'ResponseType': resp_type,\n",
    "            'MeanConfidence': mean_conf,\n",
    "            'CI_Lower': ci_lower,\n",
    "            'CI_Upper': ci_upper\n",
    "        })\n",
    "\n",
    "conf_df = pd.DataFrame(conf_stats)\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55457b21",
   "metadata": {},
   "source": [
    "By analyzing the absolute sum of confidence ratings across source dimensions, participants' subjective certainty in their judgments can be quantified. This suggests that Remember responses are associated with the highest confidence, followed by Know and then Guess responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797bb71",
   "metadata": {},
   "source": [
    "### Create a formatted Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d77f509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response Type</th>\n",
       "      <th>Both Sources Correct</th>\n",
       "      <th>One Source Correct</th>\n",
       "      <th>Neither Source Correct</th>\n",
       "      <th>Mean Summed Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember</td>\n",
       "      <td>509</td>\n",
       "      <td>510</td>\n",
       "      <td>412</td>\n",
       "      <td>3.99 (3.86, 4.13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>122</td>\n",
       "      <td>199</td>\n",
       "      <td>243</td>\n",
       "      <td>2.59 (2.42, 2.76)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guess</td>\n",
       "      <td>43</td>\n",
       "      <td>66</td>\n",
       "      <td>225</td>\n",
       "      <td>1.29 (1.12, 1.47)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Response Type  Both Sources Correct  One Source Correct  \\\n",
       "0      remember                   509                 510   \n",
       "1          know                   122                 199   \n",
       "2         guess                    43                  66   \n",
       "\n",
       "   Neither Source Correct Mean Summed Confidence  \n",
       "0                     412      3.99 (3.86, 4.13)  \n",
       "1                     243      2.59 (2.42, 2.76)  \n",
       "2                     225      1.29 (1.12, 1.47)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_data = []\n",
    "\n",
    "for resp_type in ['Remember', 'Know', 'Guess']:\n",
    "    if resp_type in freq_table.index:\n",
    "        both = freq_table.loc[resp_type, 'both_sources_correct'] if 'both_sources_correct' in freq_table.columns else 0\n",
    "        one = freq_table.loc[resp_type, 'one_source_correct'] if 'one_source_correct' in freq_table.columns else 0\n",
    "        neither = freq_table.loc[resp_type, 'neither_source_correct'] if 'neither_source_correct' in freq_table.columns else 0\n",
    "        \n",
    "        conf_row = conf_df[conf_df['ResponseType'] == resp_type]\n",
    "        if not conf_row.empty:\n",
    "            mean_conf = conf_row['MeanConfidence'].values[0]\n",
    "            ci_lower = conf_row['CI_Lower'].values[0]\n",
    "            ci_upper = conf_row['CI_Upper'].values[0]\n",
    "            \n",
    "            table_data.append({\n",
    "                'Response Type': resp_type.lower(),\n",
    "                'Both Sources Correct': int(both),\n",
    "                'One Source Correct': int(one),\n",
    "                'Neither Source Correct': int(neither),\n",
    "                'Mean Summed Confidence': f\"{mean_conf:.2f} ({ci_lower:.2f}, {ci_upper:.2f})\"\n",
    "            })\n",
    "\n",
    "# Create the formatted table\n",
    "table_1 = pd.DataFrame(table_data)\n",
    "\n",
    "table_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6dbc",
   "metadata": {},
   "source": [
    "This cell organizes the frequency data and confidence statistics into a properly formatted table matching the paper's presentation. The table clearly displays both the distribution of source accuracy across response types and the corresponding confidence ratings, making it easy to observe the systematic relationship between subjective experience, source accuracy, and confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f66e4",
   "metadata": {},
   "source": [
    "### Prepare data for ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66cfd3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova_data = []\n",
    "\n",
    "for subject in hit_trials['Subject'].unique():\n",
    "    # Get data for this subject\n",
    "    subject_data = hit_trials[hit_trials['Subject'] == subject]\n",
    "    \n",
    "    # Calculate mean confidence for each response type\n",
    "    for resp_type in ['Remember', 'Know', 'Guess']:\n",
    "        resp_data = subject_data[subject_data['ItemRecognResp'] == resp_type]\n",
    "        if len(resp_data) > 0:  # Only add if there's data\n",
    "            mean_conf = resp_data['SourceConfidence_absSum'].mean()\n",
    "            anova_data.append({\n",
    "                'Subject': subject,\n",
    "                'ResponseType': resp_type,\n",
    "                'Confidence': mean_conf\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "anova_df = pd.DataFrame(anova_data)\n",
    "\n",
    "# Check how many subjects have data for all three response types\n",
    "response_counts = anova_df.groupby('Subject').size()\n",
    "complete_subjects = response_counts[response_counts == 3].index\n",
    "complete_subject_count = len(complete_subjects)\n",
    "\n",
    "# Keep only subjects with complete data\n",
    "anova_df_balanced = anova_df[anova_df['Subject'].isin(complete_subjects)]\n",
    "\n",
    "# Display the count of subjects with complete data\n",
    "complete_subject_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a545e6e",
   "metadata": {},
   "source": [
    "### Run one-way repeated-measures ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d42738b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>ddof1</th>\n",
       "      <th>ddof2</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>p-GG-corr</th>\n",
       "      <th>ng2</th>\n",
       "      <th>eps</th>\n",
       "      <th>sphericity</th>\n",
       "      <th>W-spher</th>\n",
       "      <th>p-spher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResponseType</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>73.1084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4192</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>False</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>0.0201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Source  ddof1  ddof2        F  p-unc  p-GG-corr     ng2     eps  \\\n",
       "0  ResponseType      2     84  73.1084    0.0        0.0  0.4192  0.8521   \n",
       "\n",
       "   sphericity  W-spher  p-spher  \n",
       "0       False   0.8264   0.0201  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aov4 = pg.rm_anova(\n",
    "    data=anova_df_balanced,\n",
    "    dv='Confidence',\n",
    "    within='ResponseType',\n",
    "    subject='Subject',\n",
    "    #detailed=True\n",
    ")\n",
    "\n",
    "aov4.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a48f2",
   "metadata": {},
   "source": [
    "The highly significant result (F(2, 84) = 73.11, p < 0.0001, η²p = 0.42) confirms that confidence ratings differ substantially across Remember, Know, and Guess responses, providing statistical support for the gradient observed in the descriptive data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e311c57",
   "metadata": {},
   "source": [
    "### Run post-hoc tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ea386c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2320256645.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bf10_rk = pg.bayesfactor_ttest(rk_ttest['T'][0], len(complete_subjects), paired=True)\n",
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2320256645.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bf10_kg = pg.bayesfactor_ttest(kg_ttest['T'][0], len(complete_subjects), paired=True)\n",
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2320256645.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bf10_rg = pg.bayesfactor_ttest(rg_ttest['T'][0], len(complete_subjects), paired=True)  # Added\n",
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2320256645.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'T-value': [rk_ttest['T'][0], kg_ttest['T'][0], rg_ttest['T'][0]],  # Added rg t-value\n",
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2320256645.py:27: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'df': [rk_ttest['dof'][0], kg_ttest['dof'][0], rg_ttest['dof'][0]],  # Added rg df\n",
      "C:\\Users\\LINZAN~1\\AppData\\Local\\Temp/ipykernel_544/2320256645.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'p-value': [rk_ttest['p-val'][0] * 3, kg_ttest['p-val'][0] * 3, rg_ttest['p-val'][0] * 3],  # Bonferroni correction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison</th>\n",
       "      <th>T-value</th>\n",
       "      <th>df</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Cohen's d</th>\n",
       "      <th>BF10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember vs Know</td>\n",
       "      <td>6.069102</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.130294</td>\n",
       "      <td>4.944714e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Know vs Guess</td>\n",
       "      <td>5.194947</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.841999</td>\n",
       "      <td>3.363812e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember vs Guess</td>\n",
       "      <td>14.017758</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.175839</td>\n",
       "      <td>2.874800e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Comparison    T-value  df   p-value  Cohen's d          BF10\n",
       "0   Remember vs Know   6.069102  42  0.000001   1.130294  4.944714e+04\n",
       "1      Know vs Guess   5.194947  42  0.000017   0.841999  3.363812e+03\n",
       "2  Remember vs Guess  14.017758  42  0.000000   2.175839  2.874800e+14"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R vs K comparison\n",
    "r_data = anova_df_balanced[anova_df_balanced['ResponseType'] == 'Remember']['Confidence']\n",
    "k_data = anova_df_balanced[anova_df_balanced['ResponseType'] == 'Know']['Confidence']\n",
    "rk_ttest = pg.ttest(r_data, k_data, paired=True)\n",
    "\n",
    "# K vs G comparison\n",
    "g_data = anova_df_balanced[anova_df_balanced['ResponseType'] == 'Guess']['Confidence']\n",
    "kg_ttest = pg.ttest(k_data, g_data, paired=True)\n",
    "\n",
    "# R vs G comparison (added)\n",
    "rg_ttest = pg.ttest(r_data, g_data, paired=True)\n",
    "\n",
    "# Calculate effect sizes\n",
    "d_rk = (r_data.mean() - k_data.mean()) / np.sqrt((r_data.var() + k_data.var()) / 2)\n",
    "d_kg = (k_data.mean() - g_data.mean()) / np.sqrt((k_data.var() + g_data.var()) / 2)\n",
    "d_rg = (r_data.mean() - g_data.mean()) / np.sqrt((r_data.var() + g_data.var()) / 2)  # Added\n",
    "\n",
    "# Calculate Bayes Factors\n",
    "bf10_rk = pg.bayesfactor_ttest(rk_ttest['T'][0], len(complete_subjects), paired=True)\n",
    "bf10_kg = pg.bayesfactor_ttest(kg_ttest['T'][0], len(complete_subjects), paired=True)\n",
    "bf10_rg = pg.bayesfactor_ttest(rg_ttest['T'][0], len(complete_subjects), paired=True)  # Added\n",
    "\n",
    "# Create a summary table of post-hoc tests\n",
    "posthoc_summary = pd.DataFrame({\n",
    "    'Comparison': ['Remember vs Know', 'Know vs Guess', 'Remember vs Guess'],  # Added third comparison\n",
    "    'T-value': [rk_ttest['T'][0], kg_ttest['T'][0], rg_ttest['T'][0]],  # Added rg t-value\n",
    "    'df': [rk_ttest['dof'][0], kg_ttest['dof'][0], rg_ttest['dof'][0]],  # Added rg df\n",
    "    'p-value': [rk_ttest['p-val'][0] * 3, kg_ttest['p-val'][0] * 3, rg_ttest['p-val'][0] * 3],  # Bonferroni correction\n",
    "    'Cohen\\'s d': [d_rk, d_kg, d_rg],  # Added d_rg\n",
    "    'BF10': [bf10_rk, bf10_kg, bf10_rg]  # Added bf10_rg\n",
    "})\n",
    "\n",
    "posthoc_summary.round(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76ceb3",
   "metadata": {},
   "source": [
    "The results show strong evidence for differences between all pairs, with the Remember vs. Guess comparison showing the most dramatic difference (t(42) = 14.02, p < 0.00001, d = 2.18, BF10 = 2.87e+14). The extremely large Bayes factor for this comparison (over 287 trillion) indicates overwhelming evidence for the difference between Remember and Guess confidence ratings. The Remember vs. Know comparison also shows substantial evidence (BF10 = 49,447), while the Know vs. Guess comparison shows strong but comparatively smaller evidence (BF10 = 3,364). These results establish a clear hierarchical relationship in confidence ratings across the three response types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f66da",
   "metadata": {},
   "source": [
    "## Conclusion of table 1\n",
    "The analysis successfully replicates Table 1, demonstrating a clear relationship between subjective memory judgments, source accuracy, and confidence ratings. The data reveal three key findings:\n",
    "First, source memory accuracy varies systematically across response types. Remember responses were associated with higher source accuracy (509 trials with both sources correct) compared to Know responses (122 trials with both sources correct) and Guess responses (43 trials with both sources correct). This pattern supports the view that subjective experiences of remembering are associated with better source memory than knowing or guessing.\n",
    "Second, confidence ratings showed a similar gradient across response types, with Remember responses receiving the highest confidence ratings (M = 3.99), followed by Know responses (M = 2.59), and Guess responses (M = 1.29). The ANOVA confirmed these differences were highly significant (F(2, 84) = 73.11, p < 0.0001, η²p = 0.42).\n",
    "Third, post-hoc comparisons revealed significant differences between all response types, with particularly strong evidence for the difference between Remember and Guess responses (BF10 = 2.87e+14). This extraordinarily large Bayes factor provides overwhelming evidence that Remember and Guess responses represent distinct subjective states that differ substantially in confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c709e77",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32416a29",
   "metadata": {},
   "source": [
    "The replication of Experiment 1 from Huang & Shanks (2021) provides evidence for the relationship between processing fluency and memory for source information. Through analyses of identification reaction times, subjective memory judgments, and source accuracy, several key findings emerge. First, identification RTs systematically vary with recognition judgments, being fastest for hits and progressively slower for misses, false alarms, and correct rejections. Second, when examining subjective memory experiences, a clear gradient in identification speed was observed across Remember, Know, and Guess responses, suggesting that processing fluency correlates with the phenomenological experience of remembering. Third, source memory accuracy was directly related to processing fluency, with faster identification times for trials with correctly remembered source information compared to trials with incorrect source judgments.\n",
    "\n",
    "These findings challenge conventional dual-process accounts that assume source memory judgments rely exclusively on recollection processes. Instead, the results support a more nuanced view in which familiarity-based processes (reflected in processing fluency) contribute to source memory judgments. The significant relationships between identification speed, source accuracy, and subjective confidence ratings suggest that the memory system's operation may be better characterized by a continuous signal detection approach rather than by discrete, independent processes. This perspective aligns with theoretical accounts proposing that familiarity and recollection may operate along a continuum of memory strength rather than as qualitatively distinct systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
